{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ðŸš¨ PASTE YOUR COPIED KEY HERE, inside the quotation marks ðŸš¨\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"#########################################\"\n",
        "\n",
        "print(\"API Key set successfully in environment variable.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ws-VzWR3Fp6o",
        "outputId": "911b108d-31af-4969-cb0f-55730c25e264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key set successfully in environment variable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile verifier_agent.py\n",
        "\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from typing import Dict, List, Any, Tuple\n",
        "\n",
        "# --- API Configuration ---\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
        "\n",
        "if not API_KEY:\n",
        "    print(\"CRITICAL ERROR: GEMINI_API_KEY environment variable is not set. API calls will fail.\")\n",
        "\n",
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=\" + API_KEY\n",
        "\n",
        "def call_gemini_api_with_retry(payload, max_retries=3):\n",
        "    \"\"\"Handles API calls with basic exponential backoff.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if not API_KEY:\n",
        "                raise requests.exceptions.HTTPError(\"403 Client Error: Forbidden (API Key is missing)\")\n",
        "\n",
        "            response = requests.post(API_URL, headers={'Content-Type': 'application/json'}, json=payload)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if \"API Key is missing\" in str(e):\n",
        "                raise\n",
        "\n",
        "            if response.status_code == 429 and attempt < max_retries - 1:\n",
        "                wait_time = 2 ** attempt\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "def verify_claim_with_grounding(search_query, claim_text, top_k: int = 5) -> Tuple[str, List[Dict[str, str]]]:\n",
        "    \"\"\"\n",
        "    Calls the Gemini API, enabling the Google Search tool to verify a claim.\n",
        "    Returns a summary and a list of the top-k source dictionaries.\n",
        "\n",
        "    Args:\n",
        "        search_query: The query used to search the web.\n",
        "        claim_text: The claim to be verified.\n",
        "        top_k: The maximum number of sources to return.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the summary verdict string and the list of sources.\n",
        "    \"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are a Source Verifier Agent. Use Google Search grounding to find evidence \"\n",
        "        \"that either strongly corroborates or strongly contradicts the provided claim. \"\n",
        "        \"Summarize the findings concisely and state whether the claim is: CORROBORATED, CONTRADICTED, or UNVERIFIED. \"\n",
        "        \"Do not invent information.\"\n",
        "    )\n",
        "    user_query = f\"Verify the following claim: '{claim_text}'. Search using the query: '{search_query}'\"\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{ \"parts\": [{ \"text\": user_query }] }],\n",
        "        \"systemInstruction\": { \"parts\": [{ \"text\": system_prompt }] },\n",
        "        # CRITICAL: Enable Google Search Grounding for evidence retrieval\n",
        "        \"tools\": [{ \"google_search\": {} }],\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = call_gemini_api_with_retry(payload)\n",
        "        candidate = response.get('candidates', [{}])[0]\n",
        "\n",
        "        # 1. Extract generated text summary\n",
        "        summary = candidate.get('content', {}).get('parts', [{}])[0].get('text', 'Verification failed: No summary generated.')\n",
        "\n",
        "        # 2. Extract grounding sources (citations) and limit to top_k\n",
        "        sources = []\n",
        "        grounding_metadata = candidate.get('groundingMetadata')\n",
        "        if grounding_metadata and grounding_metadata.get('groundingAttributions'):\n",
        "            # Extract all valid sources\n",
        "            all_sources = [\n",
        "                {'uri': attr.get('web', {}).get('uri'), 'title': attr.get('web', {}).get('title')}\n",
        "                for attr in grounding_metadata['groundingAttributions']\n",
        "                if attr.get('web', {}).get('uri')\n",
        "            ]\n",
        "            # Limit the sources to top_k\n",
        "            sources = all_sources[:top_k]\n",
        "\n",
        "        return summary, sources\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        return f\"Verification failed due to API error: {e}\", []\n",
        "    except Exception as e:\n",
        "        return f\"Verification failed due to generic error: {e}\", []\n",
        "\n",
        "def normalize_verdict_text(summary):\n",
        "    \"\"\"\n",
        "    Strips common formatting (newlines, asterisks, colons, punctuation) to ensure\n",
        "    robust keyword matching for CONTRADICTED/UNVERIFIED/CORROBORATED.\n",
        "    \"\"\"\n",
        "    # Convert to upper case\n",
        "    text = summary.upper()\n",
        "    # Remove common markdown bolding, punctuation, and newlines that interfere with simple matching\n",
        "    text = text.replace('*', '').replace(':', '').replace('\\n', ' ').replace('.', ' ')\n",
        "    return text\n",
        "\n",
        "def calculate_verification_score(claims):\n",
        "    \"\"\"\n",
        "    Implements the Source Reliability Heuristic (TM 3 Step 2).\n",
        "    A high score (closer to 1.0) indicates that the claims are highly CONTRADICTED (i.e., high risk of fake).\n",
        "    \"\"\"\n",
        "    risk_count = 0\n",
        "\n",
        "    print(\"\\n--- DEBUGGING SCORING LOGIC ---\")\n",
        "    for i, claim in enumerate(claims):\n",
        "        summary = claim.get('corroboration_summary', '')\n",
        "        # Use the robust normalization function\n",
        "        normalized_verdict = normalize_verdict_text(summary)\n",
        "\n",
        "        # Print the claim type and the normalized string being processed\n",
        "        print(f\"Claim {i+1} ({claim.get('category')}): Normalized Verdict String: '{normalized_verdict[:100]}...'\")\n",
        "\n",
        "        # A claim contributes to the risk score if it is explicitly CONTRADICTED or UNVERIFIED.\n",
        "        # Check for CONTRADICTED first, as it's the highest risk factor.\n",
        "        if \"CONTRADICTED\" in normalized_verdict:\n",
        "            risk_count += 1\n",
        "            print(f\"  -> MATCH: CONTRADICTED. Risk Count: {risk_count}\")\n",
        "        elif \"UNVERIFIED\" in normalized_verdict:\n",
        "            risk_count += 1\n",
        "            print(f\"  -> MATCH: UNVERIFIED. Risk Count: {risk_count}\")\n",
        "        else:\n",
        "            print(\"  -> NO MATCH (CORROBORATED/N/A).\")\n",
        "\n",
        "    total_claims = len(claims)\n",
        "    if total_claims == 0:\n",
        "        return 0.5\n",
        "\n",
        "    # Score = proportion of risky claims (Contradicted or Unverified)\n",
        "    verification_score = risk_count / total_claims if total_claims > 0 else 0.5\n",
        "\n",
        "    print(f\"Total Claims: {total_claims}. Final Risk Count: {risk_count}. Base Score: {verification_score:.4f}\")\n",
        "\n",
        "    # Simple Contradiction Logic (TM 3 Step 3): Increase score if critical policy is contradicted\n",
        "    # Use the same normalization logic here\n",
        "    has_contradicted_policy = any(\"CONTRADICTED\" in normalize_verdict_text(c.get('corroboration_summary', '')) and c.get('category') == 'POLICY' for c in claims)\n",
        "    if has_contradicted_policy:\n",
        "        print(\"ALERT: CRITICAL POLICY CLAIM WAS CONTRADICTED. ADJUSTING SCORE.\")\n",
        "        verification_score = min(1.0, verification_score + 0.2)\n",
        "\n",
        "    return verification_score\n",
        "\n",
        "def tm3_verifier_agent(state):\n",
        "    \"\"\"\n",
        "    Implements the Evidence Retrieval and Source Reliability Agent logic.\n",
        "    \"\"\"\n",
        "    print(\"--- TM 3 (Verifier Agent): Running Evidence Retrieval with Google Search Grounding ---\")\n",
        "\n",
        "    if not state['claims']:\n",
        "        print(\"No claims extracted by TM 2. Setting Verification Score to neutral (0.5).\")\n",
        "        state['verification_score'] = 0.5\n",
        "        return state\n",
        "\n",
        "    for i, claim in enumerate(state['claims']):\n",
        "        print(f\"-> Verifying Claim {i+1} ({claim['category']}): {claim['claim_text']}\")\n",
        "\n",
        "        # Note: We keep the top_k default at 5 here. We could pass it as a parameter if needed.\n",
        "        summary, sources = verify_claim_with_grounding(claim['search_query'], claim['claim_text'], top_k=5)\n",
        "\n",
        "        # Update the claim object in the state\n",
        "        claim['corroboration_summary'] = summary\n",
        "        claim['sources'] = sources\n",
        "\n",
        "        print(f\"   Summary: {summary.splitlines()[0]}...\")\n",
        "        print(f\"   Sources Found: {len(sources)}\")\n",
        "\n",
        "    # Recalculate the overall Verification Score based on the updated claims\n",
        "    state['verification_score'] = calculate_verification_score(state['claims'])\n",
        "\n",
        "    print(f\"Final Verification Score: {state['verification_score']:.4f}\")\n",
        "    return state"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting verifier_agent.py\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZyCH0lsC2_9",
        "outputId": "dd181c0c-d3ba-4d0b-d62e-012279dc0941"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the following is for checking the evidence retrieval agent indepently"
      ],
      "metadata": {
        "id": "sZRPFgFYDxKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Ensure the core agent file can be imported\n",
        "try:\n",
        "    from verifier_agent import tm3_verifier_agent\n",
        "except ImportError:\n",
        "    print(\"Error: Could not find 'verifier_agent.py'. Make sure it is in the same directory.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "def mock_state_with_claims():\n",
        "    \"\"\"\n",
        "    Creates a mock 'AssessmentState' object that simulates the output\n",
        "    from a fully functional TM 2 (Claim Extraction Agent).\n",
        "    \"\"\"\n",
        "    print(\"--- Initializing Mock State (Simulating TM 2 Output) ---\")\n",
        "\n",
        "    # These claims simulate the JSON structure expected from TM 2\n",
        "    mock_claims = [\n",
        "        {\n",
        "            \"claim_text\": \"We are acquiring BananaCo for $500 million starting next Tuesday.\",\n",
        "            \"category\": \"FINANCIAL\",\n",
        "            \"search_query\": \"BananaCo acquisition $500 million official announcement\",\n",
        "            \"priority\": 1\n",
        "        },\n",
        "        {\n",
        "            \"claim_text\": \"Hello, this is CEO John Smith.\",\n",
        "            \"category\": \"IDENTITY\",\n",
        "            \"search_query\": \"CEO John Smith official bio\",\n",
        "            \"priority\": 2\n",
        "        },\n",
        "        {\n",
        "            \"claim_text\": \"Our policy is to never wire funds over the phone.\",\n",
        "            \"category\": \"POLICY\",\n",
        "            \"search_query\": \"Company policy on wire transfer requests via voice\",\n",
        "            \"priority\": 1\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    mock_state = {\n",
        "        \"audio_url\": \"mock://test_audio.wav\",\n",
        "        \"claimed_speaker_id\": \"John Smith\",\n",
        "        \"transcript\": \"Mock transcript content...\",\n",
        "        \"claims\": mock_claims,\n",
        "        \"verification_score\": None\n",
        "    }\n",
        "\n",
        "    return mock_state\n",
        "\n",
        "def run_tm3_isolation_test():\n",
        "    \"\"\"\n",
        "    Runs only the TM 3 agent on mock data and prints the result.\n",
        "    \"\"\"\n",
        "    state = mock_state_with_claims()\n",
        "\n",
        "    # Call your TM 3 function\n",
        "    print(\"\\n--- Executing TM 3 Verifier Agent (Evidence Retrieval) ---\")\n",
        "    final_state = tm3_verifier_agent(state)\n",
        "\n",
        "    # Print the final, verified state\n",
        "    print(\"\\n--- FINAL TM 3 OUTPUT (Verified Claims & Score) ---\")\n",
        "    print(f\"Overall Verification Score: {final_state['verification_score']:.4f}\")\n",
        "\n",
        "    # Print detailed claim verification results\n",
        "    for i, claim in enumerate(final_state['claims']):\n",
        "        print(f\"\\nClaim {i+1}: {claim['claim_text']}\")\n",
        "        print(f\"  Summary: {claim.get('corroboration_summary', 'N/A')}\")\n",
        "        print(f\"  Sources: {len(claim.get('sources', []))} found.\")\n",
        "        if claim.get('sources'):\n",
        "            print(f\"  Example Source: {claim['sources'][0]['title']} ({claim['sources'][0]['uri']})\")\n",
        "\n",
        "    # This is the final JSON structure for debugging:\n",
        "    print(\"\\nFULL STATE OUTPUT:\")\n",
        "    print(json.dumps(final_state, indent=2))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_tm3_isolation_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft0dSb3VD3R5",
        "outputId": "81fbccb5-9a00-42eb-82a3-ee19d6d43cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing Mock State (Simulating TM 2 Output) ---\n",
            "\n",
            "--- Executing TM 3 Verifier Agent (Evidence Retrieval) ---\n",
            "--- TM 3 (Verifier Agent): Executing Evidence Retrieval ---\n",
            "-> Verifying Claim 1 (FINANCIAL): We are acquiring BananaCo for $500 million starting next Tuesday.\n",
            "   Summary: The search for 'BananaCo acquisition $500 million official announcement' did not yield any results confirming an official announcement for the acquisition of BananaCo for \\$500 million starting next Tuesday....\n",
            "-> Verifying Claim 2 (IDENTITY): Hello, this is CEO John Smith.\n",
            "   Summary: Multiple individuals named John Smith are or have been chief executive officers....\n",
            "-> Verifying Claim 3 (POLICY): Our policy is to never wire funds over the phone.\n",
            "   Summary: The claim \"Our policy is to never wire funds over the phone\" is **UNVERIFIED** as a general or industry-wide policy....\n",
            "Final Verification Score: 1.0000\n",
            "\n",
            "--- FINAL TM 3 OUTPUT (Verified Claims & Score) ---\n",
            "Overall Verification Score: 1.0000\n",
            "\n",
            "Claim 1: We are acquiring BananaCo for $500 million starting next Tuesday.\n",
            "  Summary: The search for 'BananaCo acquisition $500 million official announcement' did not yield any results confirming an official announcement for the acquisition of BananaCo for \\$500 million starting next Tuesday.\n",
            "\n",
            "**Verdict: UNVERIFIED**\n",
            "  Sources: 0 found.\n",
            "\n",
            "Claim 2: Hello, this is CEO John Smith.\n",
            "  Summary: Multiple individuals named John Smith are or have been chief executive officers.\n",
            "\n",
            "**Corroborating Evidence:**\n",
            "*   A **John Smith** is the current Chief Executive Officer at Foundation Partners Group.\n",
            "*   **John A. Smith** is the president and CEO-elect of FedEx Ground and previously served as president and CEO of FedEx Freight.\n",
            "*   A **John Smith** was the chief executive officer of BBC Worldwide from 2004 until 2012.\n",
            "\n",
            "**Summary of Findings:**\n",
            "The existence of multiple real-life CEOs named John Smith strongly corroborates the plausibility of the statement's content, as it is a factually correct title for several individuals. However, the claim is a generic, uncontextualized direct quote, and there is no evidence confirming that any specific \"CEO John Smith\" has made this *exact* introduction.\n",
            "\n",
            "**VERDICT:** UNVERIFIED\n",
            "  Sources: 0 found.\n",
            "\n",
            "Claim 3: Our policy is to never wire funds over the phone.\n",
            "  Summary: The claim \"Our policy is to never wire funds over the phone\" is **UNVERIFIED** as a general or industry-wide policy.\n",
            "\n",
            "While the search results strongly suggest that refusing an unverified phone request is a critical security measure, they do not corroborate the absolute prohibition as a universal standard.\n",
            "\n",
            "**Summary of Findings:**\n",
            "\n",
            "*   **Verification is the Standard:** Many banks and companies have procedures in place, such as a \"call back procedure\" to a predetermined number, to verify the authenticity of a wire transfer request that was initiated via phone, fax, or email. This implies that transfers **are** processed after strict verification, contradicting the absolute \"never\" in the claim.\n",
            "*   **High Risk of Phone Fraud:** Best practices emphasize intense scrutiny and a two-step verification process for requests made by phone due to the risk of \"vishing\" (voice phishing) schemes by sophisticated hackers.\n",
            "*   **Written Agreements:** For financial institutions, a written wire transfer agreement is a critical document that can explicitly authorize a bank to rely on voice instructions from a customer to transfer funds, provided a commercially reasonable authentication procedure is outlined and followed.\n",
            "*   **Policy Focus is on Authentication:** The emphasis in fraud prevention guidance is on implementing stringent internal controls like dual control, segregation of duties, and calling back a known phone number to **verbally verify** a request, not necessarily on a blanket policy of never wiring funds based on any phone interaction.\n",
            "  Sources: 0 found.\n",
            "\n",
            "FULL STATE OUTPUT:\n",
            "{\n",
            "  \"audio_url\": \"mock://test_audio.wav\",\n",
            "  \"claimed_speaker_id\": \"John Smith\",\n",
            "  \"transcript\": \"Mock transcript content...\",\n",
            "  \"claims\": [\n",
            "    {\n",
            "      \"claim_text\": \"We are acquiring BananaCo for $500 million starting next Tuesday.\",\n",
            "      \"category\": \"FINANCIAL\",\n",
            "      \"search_query\": \"BananaCo acquisition $500 million official announcement\",\n",
            "      \"priority\": 1,\n",
            "      \"corroboration_summary\": \"The search for 'BananaCo acquisition $500 million official announcement' did not yield any results confirming an official announcement for the acquisition of BananaCo for \\\\$500 million starting next Tuesday.\\n\\n**Verdict: UNVERIFIED**\",\n",
            "      \"sources\": []\n",
            "    },\n",
            "    {\n",
            "      \"claim_text\": \"Hello, this is CEO John Smith.\",\n",
            "      \"category\": \"IDENTITY\",\n",
            "      \"search_query\": \"CEO John Smith official bio\",\n",
            "      \"priority\": 2,\n",
            "      \"corroboration_summary\": \"Multiple individuals named John Smith are or have been chief executive officers.\\n\\n**Corroborating Evidence:**\\n*   A **John Smith** is the current Chief Executive Officer at Foundation Partners Group.\\n*   **John A. Smith** is the president and CEO-elect of FedEx Ground and previously served as president and CEO of FedEx Freight.\\n*   A **John Smith** was the chief executive officer of BBC Worldwide from 2004 until 2012.\\n\\n**Summary of Findings:**\\nThe existence of multiple real-life CEOs named John Smith strongly corroborates the plausibility of the statement's content, as it is a factually correct title for several individuals. However, the claim is a generic, uncontextualized direct quote, and there is no evidence confirming that any specific \\\"CEO John Smith\\\" has made this *exact* introduction.\\n\\n**VERDICT:** UNVERIFIED\",\n",
            "      \"sources\": []\n",
            "    },\n",
            "    {\n",
            "      \"claim_text\": \"Our policy is to never wire funds over the phone.\",\n",
            "      \"category\": \"POLICY\",\n",
            "      \"search_query\": \"Company policy on wire transfer requests via voice\",\n",
            "      \"priority\": 1,\n",
            "      \"corroboration_summary\": \"The claim \\\"Our policy is to never wire funds over the phone\\\" is **UNVERIFIED** as a general or industry-wide policy.\\n\\nWhile the search results strongly suggest that refusing an unverified phone request is a critical security measure, they do not corroborate the absolute prohibition as a universal standard.\\n\\n**Summary of Findings:**\\n\\n*   **Verification is the Standard:** Many banks and companies have procedures in place, such as a \\\"call back procedure\\\" to a predetermined number, to verify the authenticity of a wire transfer request that was initiated via phone, fax, or email. This implies that transfers **are** processed after strict verification, contradicting the absolute \\\"never\\\" in the claim.\\n*   **High Risk of Phone Fraud:** Best practices emphasize intense scrutiny and a two-step verification process for requests made by phone due to the risk of \\\"vishing\\\" (voice phishing) schemes by sophisticated hackers.\\n*   **Written Agreements:** For financial institutions, a written wire transfer agreement is a critical document that can explicitly authorize a bank to rely on voice instructions from a customer to transfer funds, provided a commercially reasonable authentication procedure is outlined and followed.\\n*   **Policy Focus is on Authentication:** The emphasis in fraud prevention guidance is on implementing stringent internal controls like dual control, segregation of duties, and calling back a known phone number to **verbally verify** a request, not necessarily on a blanket policy of never wiring funds based on any phone interaction.\",\n",
            "      \"sources\": []\n",
            "    }\n",
            "  ],\n",
            "  \"verification_score\": 1.0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile source_reliability_agent.py\n",
        "\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from typing import Dict, List, Any, Tuple\n",
        "\n",
        "# --- API Configuration ---\n",
        "API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
        "\n",
        "if not API_KEY:\n",
        "    print(\"CRITICAL ERROR: GEMINI_API_KEY environment variable is not set. API calls may fail.\")\n",
        "\n",
        "API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=\" + API_KEY\n",
        "\n",
        "def call_gemini_api_with_retry(payload: Dict[str, Any], max_retries: int = 3) -> Dict[str, Any]:\n",
        "    \"\"\"Handles API calls with basic exponential backoff.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if not API_KEY:\n",
        "                raise requests.exceptions.HTTPError(\"403 Client Error: Forbidden (API Key is missing)\")\n",
        "\n",
        "            response = requests.post(API_URL, headers={'Content-Type': 'application/json'}, json=payload)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            if \"API Key is missing\" in str(e):\n",
        "                raise\n",
        "\n",
        "            if (response.status_code == 429 or response.status_code >= 500) and attempt < max_retries - 1:\n",
        "                wait_time = 2 ** attempt\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = 2 ** attempt\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "def normalize_verdict_text(summary: str) -> str:\n",
        "    \"\"\"Strips common formatting and punctuation for robust keyword matching.\"\"\"\n",
        "    text = summary.upper()\n",
        "    text = text.replace('*', '').replace(':', '').replace('\\n', ' ').replace('.', ' ')\n",
        "    return text\n",
        "\n",
        "def tm3_source_reliability_agent(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    LangGraph Node for Source Reliability Analysis (TM 3 - Step 2).\n",
        "    Uses LLM reasoning to adjust the base score based on evidence quality.\n",
        "    \"\"\"\n",
        "    print(\"--- TM 3 (Source Reliability Agent): Executing Score Adjustment ---\")\n",
        "\n",
        "    if not state.get('reliability_analysis_required'):\n",
        "        print(\"Skipping reliability analysis (no claims to process).\")\n",
        "        return state\n",
        "\n",
        "    initial_score = state.get('verification_score', 0.5)\n",
        "\n",
        "    # 1. Identify claims that have a clear verdict (CORROBORATED or CONTRADICTED) AND sources\n",
        "    citable_claims = []\n",
        "    for claim in state['claims']:\n",
        "        verdict = normalize_verdict_text(claim.get('corroboration_summary', '')).strip()\n",
        "\n",
        "        # Only analyze claims where the verdict is NOT UNVERIFIED, and sources exist\n",
        "        if (\"CORROBORATED\" in verdict or \"CONTRADICTED\" in verdict) and claim.get('sources'):\n",
        "             citable_claims.append({\n",
        "                \"claim\": claim['claim_text'],\n",
        "                \"verdict\": claim['corroboration_summary'],\n",
        "                \"sources\": claim['sources']\n",
        "            })\n",
        "\n",
        "    if not citable_claims:\n",
        "        print(\"Source Reliability: No high-confidence, citable claims found for score adjustment.\")\n",
        "        return state\n",
        "\n",
        "    # 2. Format the prompt for the LLM Analyst\n",
        "    prompt = (\n",
        "        \"You are a Source Reliability Analyst. Your task is to analyze the provided claim verification summaries \"\n",
        "        \"and their supporting sources (URIs/Titles). For each claim, determine the overall reliability of the evidence. \"\n",
        "        \"If the sources are official (e.g., government, major press, corporate press release), reliability is HIGH. \"\n",
        "        \"If sources are weak (e.g., forums, personal blogs, low-tier news), reliability is LOW. \"\n",
        "        \"Analyze the quality of the evidence for the following claims: \"\n",
        "        f\"{json.dumps(citable_claims, indent=2)}\"\n",
        "        \"\\n\\nBased on the quality of ALL analyzed evidence, provide ONE final verdict (HIGH, MEDIUM, or LOW) for the overall reliability of the combined evidence.\"\n",
        "    )\n",
        "\n",
        "    payload = {\n",
        "        \"contents\": [{ \"parts\": [{ \"text\": prompt }] }],\n",
        "        \"systemInstruction\": { \"parts\": [{ \"text\": \"Analyze source reliability and provide a single word verdict: HIGH, MEDIUM, or LOW.\" }] },\n",
        "    }\n",
        "\n",
        "    # 3. Call the LLM Analyst\n",
        "    try:\n",
        "        response = call_gemini_api_with_retry(payload)\n",
        "        llm_verdict = response.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '').upper().strip()\n",
        "\n",
        "        adjustment = 0.0\n",
        "\n",
        "        if \"LOW\" in llm_verdict:\n",
        "            # Low quality sources makes the verification verdict less trustworthy -> increase risk\n",
        "            adjustment = 0.05\n",
        "            print(f\"Source Reliability Verdict: LOW. Applying +{adjustment:.2f} risk adjustment.\")\n",
        "        elif \"HIGH\" in llm_verdict:\n",
        "            # High quality sources makes the verification verdict highly trustworthy -> decrease risk\n",
        "            adjustment = -0.05\n",
        "            print(f\"Source Reliability Verdict: HIGH. Applying {adjustment:.2f} risk adjustment.\")\n",
        "        else:\n",
        "            print(\"Source Reliability Verdict: MEDIUM (No adjustment).\")\n",
        "\n",
        "        final_score = max(0.0, min(1.0, initial_score + adjustment))\n",
        "        state['verification_score'] = final_score\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Source Reliability analysis failed: {e}. Keeping initial score: {initial_score:.4f}.\")\n",
        "        # If the LLM call fails, we revert to the base score\n",
        "        state['verification_score'] = initial_score\n",
        "\n",
        "    print(f\"Final Verification Score (Adjusted): {state['verification_score']:.4f}\")\n",
        "    return state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4CGxelfYjtN",
        "outputId": "2706d7ad-5735-4e44-a6c0-6e7729e26ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing source_reliability_agent.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ci8gHFZ-EhzS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}